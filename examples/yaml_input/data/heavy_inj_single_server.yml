# ───────────────────────────────────────────────────────────────
# AsyncFlow scenario (HEAVY): generator ➜ client ➜ server ➜ client
# Edge-latency spike + heavier workload to provoke queue growth.
# ───────────────────────────────────────────────────────────────

# 1) Traffic generator (heavier load)
rqs_input:
  id: rqs-1
  # More concurrent users and higher per-user rate drive the system harder.
  avg_active_users: { mean: 300 }
  avg_request_per_minute_per_user: { mean: 30 }
  user_sampling_window: 60

# 2) Topology
topology_graph:
  nodes:
    client: { id: client-1 }
    servers:
      - id: srv-1
        # Keep just 1 CPU core so the server becomes a bottleneck.
        server_resources: { cpu_cores: 1, ram_mb: 8000 }
        endpoints:
          - endpoint_name: ep-1
            probability: 1.0
            steps:
              # Heavier CPU (~5 ms) to increase service time
              - kind: initial_parsing
                step_operation: { cpu_time: 0.005 }
              # Larger working set to keep RAM busy
              - kind: ram
                step_operation: { necessary_ram: 200 }
              # Longer I/O wait (~200 ms) to create a noticeable I/O queue
              - kind: io_wait
                step_operation: { io_waiting_time: 0.2 }

  edges:
    - id: gen-to-client
      source: rqs-1
      target: client-1
      latency: { mean: 0.003, distribution: exponential }

    - id: client-to-server
      source: client-1
      target: srv-1
      latency: { mean: 0.003, distribution: exponential }

    - id: server-to-client
      source: srv-1
      target: client-1
      latency: { mean: 0.003, distribution: exponential }

# 3) Simulation settings
sim_settings:
  # Longer horizon so we clearly see pre-/during-/post-spike behavior.
  total_simulation_time: 600
  sample_period_s: 0.05
  enabled_sample_metrics:
    - ready_queue_len
    - event_loop_io_sleep
    - ram_in_use
    - edge_concurrent_connection
  enabled_event_metrics:
    - rqs_clock

# 4) Events (validated by Pydantic)
# Large deterministic edge spike (+3.0 s) during [180, 300] s on the
# client→server edge. With the heavier workload, this should help
# exacerbate queue growth/oscillations around the spike window.
events:
  - event_id: ev-spike-heavy
    target_id: client-to-server
    start:
      kind: network_spike_start
      t_start: 180.0
      spike_s: 3.0
    end:
      kind: network_spike_end
      t_end: 300.0
