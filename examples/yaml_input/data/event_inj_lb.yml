# AsyncFlow SimulationPayload — LB + 2 servers (medium load) with events
#
# Topology:
#   generator → client → LB → srv-1
#                              └→ srv-2
#   srv-1 → client
#   srv-2 → client
#
# Workload targets ~40 rps (120 users × 20 req/min ÷ 60).

rqs_input:
  id: rqs-1
  avg_active_users: { mean: 120 }
  avg_request_per_minute_per_user: { mean: 20 }
  user_sampling_window: 60

topology_graph:
  nodes:
    client: { id: client-1 }

    load_balancer:
      id: lb-1
      algorithms: round_robin
      server_covered: [srv-1, srv-2]

    servers:
      - id: srv-1
        server_resources: { cpu_cores: 1, ram_mb: 2048 }
        endpoints:
          - endpoint_name: /api
            steps:
              - kind: initial_parsing
                step_operation: { cpu_time: 0.002 }         # 2 ms CPU
              - kind: ram
                step_operation: { necessary_ram: 128 }      # 128 MB
              - kind: io_wait
                step_operation: { io_waiting_time: 0.012 }  # 12 ms I/O wait

      - id: srv-2
        server_resources: { cpu_cores: 1, ram_mb: 2048 }
        endpoints:
          - endpoint_name: /api
            steps:
              - kind: initial_parsing
                step_operation: { cpu_time: 0.002 }
              - kind: ram
                step_operation: { necessary_ram: 128 }
              - kind: io_wait
                step_operation: { io_waiting_time: 0.012 }

  edges:
    - { id: gen-client,  source: rqs-1,   target: client-1, latency: { mean: 0.003, distribution: exponential } }
    - { id: client-lb,   source: client-1, target: lb-1,    latency: { mean: 0.002, distribution: exponential } }
    - { id: lb-srv1,     source: lb-1,    target: srv-1,    latency: { mean: 0.002, distribution: exponential } }
    - { id: lb-srv2,     source: lb-1,    target: srv-2,    latency: { mean: 0.002, distribution: exponential } }
    - { id: srv1-client, source: srv-1,   target: client-1, latency: { mean: 0.003, distribution: exponential } }
    - { id: srv2-client, source: srv-2,   target: client-1, latency: { mean: 0.003, distribution: exponential } }

sim_settings:
  total_simulation_time: 600
  sample_period_s: 0.05
  enabled_sample_metrics:
    - ready_queue_len
    - event_loop_io_sleep
    - ram_in_use
    - edge_concurrent_connection
  enabled_event_metrics:
    - rqs_clock

# Events:
# - Edge spikes (added latency in seconds) that stress different paths at different times.
# - Server outages that never overlap (so at least one server stays up).
events:
  # Edge spike: client → LB gets +15 ms from t=100s to t=160s
  - event_id: ev-spike-1
    target_id: client-lb
    start: { kind: network_spike_start, t_start: 100.0, spike_s: 0.015 }
    end:   { kind: network_spike_end,   t_end:   160.0 }

  # Server outage: srv-1 down from t=180s to t=240s
  - event_id: ev-srv1-down
    target_id: srv-1
    start: { kind: server_down, t_start: 180.0 }
    end:   { kind: server_up,   t_end:   240.0 }

  # Edge spike focused on srv-2 leg (LB → srv-2) from t=300s to t=360s (+20 ms)
  - event_id: ev-spike-2
    target_id: lb-srv2
    start: { kind: network_spike_start, t_start: 300.0, spike_s: 0.020 }
    end:   { kind: network_spike_end,   t_end:   360.0 }

  # Server outage: srv-2 down from t=360s to t=420s (starts right after the spike ends)
  - event_id: ev-srv2-down
    target_id: srv-2
    start: { kind: server_down, t_start: 360.0 }
    end:   { kind: server_up,   t_end:   420.0 }

  # Late spike on generator → client from t=480s to t=540s (+10 ms)
  - event_id: ev-spike-3
    target_id: gen-client
    start: { kind: network_spike_start, t_start: 480.0, spike_s: 0.010 }
    end:   { kind: network_spike_end,   t_end:   540.0 }
